{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deploy and serving Deep Learning model with TensorFlow Serving  \n",
    "## Tensorflow Extended and Tensorflow Serving \n",
    "TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines.\n",
    "\n",
    "\n",
    "![Tensorflow Extended](https://www.tensorflow.org/site-assets/images/marketing/learn/tfx-hero.svg)\n",
    "\n",
    "**How it works**\n",
    "\n",
    "> When you’re ready to move your models from research to production, use TFX to create and manage a production pipeline\n",
    ">> https://www.tensorflow.org/tfx\n",
    "\n",
    "When you’re ready to go beyond training a single model, or ready to put your amazing model to work and move it to production, TFX is there to help you build a complete ML pipeline.\n",
    "\n",
    "A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. That includes modeling, training, serving inference, and managing deployments to online, native mobile, and JavaScript targets. \n",
    "\n",
    "\n",
    "There are many great components in the pipeline. But for this tutorial I will focus on **Tensorflow Serving**. One of the most important and interesting component of TFX.\n",
    "\n",
    "### So, what is **Tensorflow Serving**? \n",
    "\n",
    "\n",
    "Machine Learning (ML) serving systems need to support **model versioning** (for model updates with a rollback option) and **multiple models** (for experimentation via A/B testing), while ensuring that concurrent models achieve high throughput on hardware accelerators (GPUs and TPUs) with low latency. TensorFlow Serving has proven performance handling tens of millions of inferences per second at Google.\n",
    "\n",
    "![Tensorflow Serving](https://www.tensorflow.org/tfx/serving/images/serving_architecture.svg)\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out of the box integration with TensorFlow models, but can be easily extended to serve other types of models.\n",
    "\n",
    "#### Key concept:\n",
    "\n",
    "* Servables: Servables are the central abstraction in TensorFlow Serving. Servables are the underlying objects that clients use to perform computation (for example, a lookup or inference).\n",
    "\n",
    "    \n",
    "* Loader: Loaders manage a servable's life cycle. The Loader API enables common infrastructure independent from specific learning algorithms, data or product use-cases involved. Specifically, Loaders standardize the APIs for loading and unloading a servable.\n",
    "\n",
    "\n",
    "* Source: Sources are plugin modules that find and provide servables. Each Source provides zero or more servable streams. For each servable stream, a Source supplies one Loader instance for each version it makes available to be loaded. (A Source is actually chained together with zero or more SourceAdapters, and the last item in the chain emits the Loaders.\n",
    "\n",
    "\n",
    "\n",
    "* Manager: Managers listen to Sources and track all versions. The Manager tries to fulfill Sources' requests, but may refuse to load an aspired version if, say, required resources aren't available. \n",
    "\n",
    "#### Tensorflow Serving vs Python Flask vs Django vs other\n",
    "\n",
    "What makes Tensorflow Serving is huge more advance than other web application framework like Python Flask or Django?\n",
    "\n",
    "When deploying a machine learning model to production, we go through these steps:\n",
    "\n",
    "1. Build web application (Flask, Django, ..)\n",
    "\n",
    "2. Create API endpoint to handle the request and communicate with backend.\n",
    "\n",
    "3. Load pretrain model\n",
    "\n",
    "4. Pre-processing, predict\n",
    "\n",
    "5. Return results to client\n",
    "\n",
    "Example of a Python Flask ***app.py***:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Python Flask **app.py**, handle request and parse result from Text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from flask import Flask, request, render_template\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "APP_ROOT = os.path.dirname(os.path.abspath(__file__))\n",
    "IMAGES_FOLDER = \"flask_images\"\n",
    "rand_str = lambda n: \"\".join([random.choice(string.ascii_letters + string.digits) for _ in range(n)])\n",
    "\n",
    "model = None\n",
    "word2vec = None\n",
    "max_length_sentences = 0\n",
    "max_length_word = 0\n",
    "num_classes = 0\n",
    "categories = None\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"main.html\")\n",
    "\n",
    "@app.route(\"/input\")\n",
    "def new_input():\n",
    "    return render_template(\"input.html\")\n",
    "\n",
    "@app.route(\"/show\", methods=[\"POST\"])\n",
    "def show():\n",
    "    global model, dictionary, max_length_word, max_length_sentences, num_classes, categories\n",
    "    trained_model = request.files[\"model\"]\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.load(trained_model)\n",
    "    else:\n",
    "        model = torch.load(trained_model, map_location=lambda storage, loc: storage)\n",
    "    dictionary = pd.read_csv(filepath_or_buffer=request.files[\"word2vec\"], header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                             usecols=[0]).values\n",
    "    dictionary = [word[0] for word in dictionary]\n",
    "    max_length_sentences = model.max_sent_length\n",
    "    max_length_word = model.max_word_length\n",
    "    num_classes = list(model.modules())[-1].out_features\n",
    "    if \"classes\" in request.files:\n",
    "        df = pd.read_csv(request.files[\"classes\"], header=None)\n",
    "        categories = [item[0] for item in df.values]\n",
    "    return render_template(\"input.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/result\", methods=[\"POST\"])\n",
    "def result():\n",
    "    global dictionary, model, max_length_sentences, max_length_word, categories\n",
    "    text = request.form[\"message\"]\n",
    "    document_encode = [\n",
    "        [dictionary.index(word) if word in dictionary else -1 for word in word_tokenize(text=sentences)] for sentences\n",
    "        in sent_tokenize(text=text)]\n",
    "\n",
    "    for sentences in document_encode:\n",
    "        if len(sentences) < max_length_word:\n",
    "            extended_words = [-1 for _ in range(max_length_word - len(sentences))]\n",
    "            sentences.extend(extended_words)\n",
    "\n",
    "    if len(document_encode) < max_length_sentences:\n",
    "        extended_sentences = [[-1 for _ in range(max_length_word)] for _ in\n",
    "                              range(max_length_sentences - len(document_encode))]\n",
    "        document_encode.extend(extended_sentences)\n",
    "\n",
    "    document_encode = [sentences[:max_length_word] for sentences in document_encode][\n",
    "                      :max_length_sentences]\n",
    "\n",
    "    document_encode = np.stack(arrays=document_encode, axis=0)\n",
    "    document_encode += 1\n",
    "    empty_array = np.zeros_like(document_encode, dtype=np.int64)\n",
    "    input_array = np.stack([document_encode, empty_array], axis=0)\n",
    "    feature = torch.from_numpy(input_array)\n",
    "    if torch.cuda.is_available():\n",
    "        feature = feature.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model._init_hidden_state(2)\n",
    "        prediction = model(feature)\n",
    "    prediction = F.softmax(prediction)\n",
    "    max_prob, max_prob_index = torch.max(prediction, dim=-1)\n",
    "    prob = \"{:.2f} %\".format(float(max_prob[0])*100)\n",
    "    if categories != None:\n",
    "        category = categories[int(max_prob_index[0])]\n",
    "    else:\n",
    "        category = int(max_prob_index[0]) + 1\n",
    "    return render_template(\"result.html\", text=text, value=prob, index=category)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.secret_key = os.urandom(12)\n",
    "    app.run(host=\"0.0.0.0\", port=4555, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![Flask result](https://github.com/uvipen/Hierarchical-attention-networks-pytorch/blob/master/demo/video.gif?raw=true)\n",
    "\n",
    "\n",
    "Flask is fine only if you are planning to demo your model on local machine, but when deploy your model to production, there  will be some issues:\n",
    "\n",
    "1. Loading and Serving model are processing inside backend codebase. Everytime clients send a request to server, pretrain model is reloaded. For one single model, reload the pretrain model is acceptable but it will be impossible to load multiple complex models at the same time (eg: object detection + image alignment + object tracking)\n",
    "\n",
    "2. Model version: There is no information about model version. Anytime you want to update your model, you need to create a new API endpoint to process or overwrite the old version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Serving is the way to solve the Python Flask disadvantages\n",
    "### Example with Tensorflow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Simple NN with Mnist datasaet\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "def make_model(input_shape=[28, 28, 1]):\n",
    "    model = tf_models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    for no_filter in [16, 32, 64]:\n",
    "        model.add(layers.Conv2D(\n",
    "            no_filter,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D(\n",
    "            pool_size=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = make_model()\n",
    "print(model.inputs, model.outputs, model.count_params())\n",
    "# [<tf.Tensor 'input_1:0' shape=(?, 28, 28, 1) dtype=float32>]\n",
    "# [<tf.Tensor 'dense_1/Softmax:0' shape=(?, 10) dtype=float32>]\n",
    "# 156234\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, save and load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# fit model and save weight\n",
    "model.fit(...)\n",
    "model.save(...)\n",
    "\n",
    "# load pretrained model \n",
    "model = load_model('./temp_models/mnist_all.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set learning_phase = 0 to change to evaluation mode: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The export path contains the name and the version of the model\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)  # Ignore dropout at inference\n",
    "export_path = './temp_models/serving/1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert h5 to Tensorflow Serving format saved_model.pb with method .simple_saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.keras.backend.get_session() as sess:\n",
    "    tf.saved_model.simple_save(\n",
    "        sess,\n",
    "        export_path,\n",
    "       inputs={'input_image': model.input},\n",
    "        outputs={'y_pred': model.output})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model by SaveModelBuilder method with custom MetaGraphDef. Custom tag-set or define assets (external file for serving)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "\n",
    "builder = saved_model_builder.SavedModelBuilder(export_dir_path)\n",
    "\n",
    "signature = predict_signature_def(\n",
    "    inputs={\n",
    "        'input_image': model.inputs[0],\n",
    "    },\n",
    "    outputs={\n",
    "        'y_pred': model.outputs[0]\n",
    "    }\n",
    ")\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=sess,\n",
    "        tags=[tag_constants.SERVING],\n",
    "        signature_def_map={'reid-predict': signature},\n",
    "        # or\n",
    "        # signature_def_map={signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},\n",
    "    )\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model with ***checkpoint*** format .cpkt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "trained_checkpoint_prefix = './temp_models/model.ckpt-00001'\n",
    "export_dir = os.path.join('./temp_models/serving', '1')\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    # Restore from checkpoint\n",
    "    loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\n",
    "    loader.restore(sess, trained_checkpoint_prefix)\n",
    "\n",
    "    # Export checkpoint to SavedModel\n",
    "    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tf.saved_model.TRAINING, tf.saved_model.SERVING],\n",
    "                                         strip_default_attrs=True)\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* File **saved_model.pb** and **variables** folder will be created: \n",
    "    - saved_model.pb: serialized model, stored graph info of the model and other metadata such as: signature, model inputs/outputs.\n",
    "    - variables: store serialized variables of the graph (learned weight)\n",
    "    \n",
    "    \n",
    "* **Tensorflow Serving manage the model version control by folder name. I.e: version 1 is folder 1**\n",
    ">temp_models/serving/1 \n",
    "\n",
    "    >├── saved_model.pb\n",
    "\n",
    "    >└── variables\n",
    "\n",
    "            >├── variables.data-00000-of-00001\n",
    "\n",
    "            >└── variables.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Use <code>save_model_cli</code> to show <code> saved_model.pb </code> metadata\n",
    "    ```bash saved_model_cli show --dir temp_models/serving/1 --tag_set serve --signature_def serving_default```\n",
    "    \n",
    "* **Result**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "  signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['input_image'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 28, 28, 1)\n",
    "        name: input_1:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['pred'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 10)\n",
    "        name: dense_1/Softmax:0\n",
    "  Method name is: tensorflow/serving/predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Test output with 1 sample**\n",
    "``` saved_model_cli run --dir temp_models/serving/1/ --tag_set serve --signature_def serving_default --input_exprs \"input_image=np.zeros((1, 28, 28, 1))\"```\n",
    "\n",
    "* **Output**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#output\n",
    "Result for output key y_pred:\n",
    "[[1.5933816e-01 1.6137624e-01 4.8642698e-05 8.6862819e-05 2.8394745e-05\n",
    "  1.3426773e-03 2.7080998e-03 6.2681846e-03 1.3640945e-02 6.5516180e-01]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gRPC (Google Remote Procedures Calls) vs RESTful (Representational State Transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensorflow Serving support both **gRPC** and **http**. To make the request to tensorflow server via gRPC, we need to install <code>tensorflow_model_server</code> and lib <code>tensorflow-serving-api</code>\n",
    "\n",
    "```bash \n",
    "\n",
    "echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
    "\n",
    "# step 2\n",
    "apt-get update && apt-get install tensorflow-model-server\n",
    "# or apt-get upgrade tensorflow-model-server\n",
    "\n",
    "# step 3\n",
    "pip install tensorflow-serving-api \n",
    "```\n",
    "\n",
    "* **Run the server:** \n",
    "```bash\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=mnist-serving --model_base_path=/home/thuc/project/tensorflow/temp_models/serving\n",
    "```\n",
    "\n",
    "\n",
    "* **Save_model folder structure. I.e: home/thuc/project/tensorflow/temp_models/serving** with 2 diffirent version\n",
    "\n",
    "<code>\n",
    "temp_models/serving\n",
    "├── 1\n",
    "│   ├── saved_model.pb\n",
    "│   └── variables\n",
    "│       ├── variables.data-00000-of-00001\n",
    "│       └── variables.index\n",
    "└── 2\n",
    "    ├── saved_model.pb\n",
    "    └── variables\n",
    "        ├── variables.data-00000-of-00001\n",
    "        └── variables.index\n",
    "...\n",
    "4 directories, 6 files\n",
    "\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restful API example, default port=8500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to request Restful API \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(x_test.shape)\n",
    "# (10000, 28, 28, 1)\n",
    "\n",
    "def rest_infer(imgs,\n",
    "               model_name='mnist-serving',\n",
    "               host='localhost',\n",
    "               port=8501,\n",
    "               signature_name=\"serving_default\"):\n",
    "    \"\"\"MNIST - serving with http - RESTful API\n",
    "    \"\"\"\n",
    "\n",
    "    if imgs.ndim == 3:\n",
    "        imgs = np.expand_dims(imgs, axis=0)\n",
    "        \n",
    "    data = json.dumps({\n",
    "        \"signature_name\": signature_name,\n",
    "        \"instances\": imgs.tolist()\n",
    "    })\n",
    "    \n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    json_response = requests.post(\n",
    "        'http://{}:{}/v1/models/{}:predict'.format(host, port, model_name),\n",
    "        data=data,\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if json_response.status_code == 200:\n",
    "        y_pred = json.loads(json_response.text)['predictions']\n",
    "        y_pred = np.argmax(y_pred, axis=-1)\n",
    "        return y_pred\n",
    "    else:\n",
    "        return None\n",
    "       \n",
    "y_pred = rest_infer(x_test)\n",
    "print(\n",
    "    accuracy_score(np.argmax(y_test, axis=-1), y_pred),\n",
    "    f1_score(np.argmax(y_test, axis=-1), y_pred, average=\"macro\")\n",
    ")\n",
    "# result\n",
    "# 0.9947 0.9946439344333233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gRPC example, default port=8500, require: model_name, signature_name,host, port, input_name, output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gRPC, default port = 8500, code require: model_name, signature_name, host, port, input_name, output_name\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import grpc\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel(\"localhost:8500\")\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "request = predict_pb2.PredictRequest()\n",
    "# model_name\n",
    "request.model_spec.name = \"mnist-serving\"\n",
    "# signature name, default is `serving_default`\n",
    "request.model_spec.signature_name = \"serving_default\"\n",
    "\n",
    "def grpc_infer(imgs):\n",
    "    \"\"\"MNIST - serving with gRPC\n",
    "    \"\"\"\n",
    "    \n",
    "    if imgs.ndim == 3:\n",
    "        imgs = np.expand_dims(imgs, axis=0)\n",
    "    \n",
    "    request.inputs[\"input_image\"].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(\n",
    "            imgs,\n",
    "            dtype=np.float32,\n",
    "            shape=imgs.shape\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        result = stub.Predict(request, 10.0)\n",
    "        result = result.outputs[\"y_pred\"].float_val\n",
    "        result = np.array(result).reshape((-1, 10))\n",
    "        result = np.argmax(result, axis=-1)\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "y_pred = grpc_infer(x_test)\n",
    "print(\n",
    "    accuracy_score(np.argmax(y_test, axis=-1), y_pred),\n",
    "    f1_score(np.argmax(y_test, axis=-1), y_pred, average=\"macro\")\n",
    ")\n",
    "# result\n",
    "# 0.9947 0.9946439344333233\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "* **Benchmark inference time between gRPC and RESTful API, with 1 request**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http\n",
    "start = time.time()\n",
    "y_pred = rest_infer(x_test[0])\n",
    "print(\"Inference time: {}\".format(time.time() - start))\n",
    "# >>> Inference time: 0.0028078556060791016\n",
    "\n",
    "# gRPC\n",
    "start = time.time()\n",
    "y_pred = grpc_infer(x_test[0])\n",
    "print(\"Inference time: {}\".format(time.time() - start))\n",
    "# >>> Inference time: 0.0012249946594238281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Inference time with 10000 MNIST sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "y_pred = rest_infer(x_test)\n",
    "print(\">>> Inference time: {}\".format(time.time() - start))\n",
    ">>> Inference time: 6.681854248046875\n",
    "\n",
    "start = time.time()\n",
    "y_pred = grpc_infer(x_test)\n",
    "print(\">>> Inference time: {}\".format(time.time() - start))\n",
    ">>> Inference time: 0.3771860599517822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gRPC has almost 18 times faster with 10000 Mnist sample request\n",
    "* With more complicate model or model with multiple input, output, **gRPC** perform even more faster than **http** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve model with multiple inputs: \n",
    "\n",
    "* Example: Face verification system. We will have 2 images as inputs, the system will parse the result if 2 images are show the same person or not \n",
    "    - Model: Siamese network\n",
    "    - Input: 2 images as inputs \n",
    "    - Output: 1 verification result\n",
    "    \n",
    "    \n",
    "![Siamese model](https://miro.medium.com/max/2524/1*8Nsq1BYQCuj9giAwltDubQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('sianet.h5')\n",
    "print(model.inputs, model.outputs)\n",
    "# output\n",
    "# <tf.Tensor 'input_6:0' shape=(?, 64, 32, 3) dtype=float32>,\n",
    "# <tf.Tensor 'input_7:0' shape=(?, 64, 32, 3) dtype=float32>],\n",
    "# <tf.Tensor 'dense_2/Sigmoid:0' shape=(?, 1) dtype=float32>])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to .pb format \n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def export_pb(export_dir_path, model):\n",
    "    builder = saved_model_builder.SavedModelBuilder(export_dir_path)\n",
    "\n",
    "    signature = predict_signature_def(\n",
    "        inputs={\n",
    "            'img1': model.inputs[0],\n",
    "            'img2': model.inputs[1]\n",
    "        },\n",
    "        outputs={\n",
    "            'predict': model.outputs[0]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    with K.get_session() as sess:\n",
    "        builder.add_meta_graph_and_variables(\n",
    "            sess=sess,\n",
    "            tags=[tag_constants.SERVING],\n",
    "            signature_def_map={'signature-reid': signature}\n",
    "        )\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=siamese-reid --model_base_path=relative-path-to-model-version\n",
    "!curl localhost:8501/v1/models/siamese-reid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request gPRC server **2 inputs**, **1 output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _grpc_client_request(\n",
    "    img1,\n",
    "    img2,\n",
    "    host='localhost',\n",
    "    port=8500,\n",
    "    img1_name='img1',\n",
    "    img2_name='img2',\n",
    "    model_spec_name='siamese-reid',\n",
    "    model_sig_name='signature-reid',\n",
    "    timeout=10\n",
    "):\n",
    "\n",
    "    host = host.replace(\"http://\", \"\").replace(\"https://\", \"\")\n",
    "    channel = grpc.insecure_channel(\"{}:{}\".format(host, port))\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "    # Create PredictRequest ProtoBuf from image data\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_spec_name\n",
    "    request.model_spec.signature_name = model_sig_name\n",
    "\n",
    "    # img1\n",
    "    img_arr1 = np.expand_dims(img1, axis=0)\n",
    "    request.inputs[img1_name].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(\n",
    "            img_arr1,\n",
    "            dtype=np.float32,\n",
    "            shape=[*img_arr1.shape]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # img2\n",
    "    img_arr2 = np.expand_dims(img2, axis=0)\n",
    "    request.inputs[img2_name].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(\n",
    "            img_arr2,\n",
    "            dtype=np.float32,\n",
    "            shape=[*img_arr2.shape]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(img_arr1.shape, img_arr2.shape)\n",
    "    \n",
    "    start = time.time()\n",
    "    # Call the TFServing Predict API\n",
    "    predict_response = stub.Predict(request, timeout=timeout)\n",
    "    print(\">>> Inference time: {}'s\".format(time.time() - start))\n",
    "    \n",
    "    return predict_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64, 32)\n",
    "img1_fp = 'path-to-img1'\n",
    "img2_fp = 'path-to-img2'\n",
    "# preprocess images \n",
    "img1 = preprocess_reid(img1_fp, img_size)\n",
    "img2 = preprocess_reid(img2_fp, img_size)\n",
    "\n",
    "# parse result\n",
    "result = _grpc_client_request(img1, img2)\n",
    "pred = np.array(result.outputs['predict'].float_val)\n",
    "pred = (pred >= 0.5).astype(int)\n",
    "print(pred)\n",
    "# [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result](https://i.imgur.com/G5UwKPT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with complex output\n",
    "\n",
    "* Object Detection model and Image Segmentation are the model with very complex output. Usually the model output flatten, normalize array content lot of coordinator, bounding-boxes, detection-boxes, detection classes, detection score, num_detections and a lot more information. First, I will go through Object Detection with **ssd-mobilenet-v2** model:\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "\n",
    "```\n",
    "ssd_mobilenet_v2_coco_2018_03_29/\n",
    "├── checkpoint\n",
    "├── frozen_inference_graph.pb\n",
    "├── model.ckpt.data-00000-of-00001\n",
    "├── model.ckpt.index\n",
    "├── model.ckpt.meta\n",
    "└── saved_model\n",
    "    ├── saved_model.pb\n",
    "    └── variables\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir /home/thuc/Downloads/pretrained_models/ssd_mobilenet_v2_coco_2018_03_29/saved_model/1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['inputs'] tensor_info:\n",
    "        dtype: DT_UINT8\n",
    "        shape: (-1, -1, -1, 3)\n",
    "        name: image_tensor:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['detection_boxes'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 100, 4)\n",
    "        name: detection_boxes:0\n",
    "    outputs['detection_classes'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 100)\n",
    "        name: detection_classes:0\n",
    "    outputs['detection_scores'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 100)\n",
    "        name: detection_scores:0\n",
    "    outputs['num_detections'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1)\n",
    "        name: num_detections:0\n",
    "  Method name is: tensorflow/serving/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model outputs include:\n",
    "    - Signature: signature_def['serving_default']\n",
    "    - Model input: dtype: int8, 3 channels, undefined input dimension\n",
    "    - Model output: \n",
    "            - detection_boxes: shape: (-1, 100, 4)\n",
    "            - detection_classes: shape: (-1, 100)\n",
    "            - detection_scores: shape: (-1, 100)\n",
    "            - num_detections: shape: (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start tensorflow_model_server\n",
    "!tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=ssd-mbv2-coco --model_base_path=/home/thuc/Downloads/ssd_mobilenet_v1_coco_2018_01_28/saved_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request gRPC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_img = \"/home/thuc/Downloads/cat.jpg\"\n",
    "img = cv2.imread(test_img)[:, :, ::-1]\n",
    "img_arr = np.expand_dims(img, axis=0)\n",
    "\n",
    "# init channel\n",
    "channel = grpc.insecure_channel(\"localhost:8500\")\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = \"ssd-mbv2-coco\"\n",
    "request.model_spec.signature_name = \"serving_default\"\n",
    "\n",
    "request.inputs[\"inputs\"].CopyFrom(\n",
    "    tf.contrib.util.make_tensor_proto(\n",
    "        img_arr,\n",
    "        dtype=np.uint8,\n",
    "        shape=img_arr.shape\n",
    "    )\n",
    ")\n",
    "result = stub.Predict(request, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse result\n",
    "* Use function provided by TF Object Dectection API: https://github.com/tensorflow/models/tree/master/research/object_detection/utils\n",
    "* File map label by TF API: https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt \n",
    "* Because the detection output of Tensorflow-serving has been flatten and normalize to [0,1] so we need to convert back to original coordinator value in order to visualize on output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "boxes = result.outputs['detection_boxes'].float_val\n",
    "classes = result.outputs['detection_classes'].float_val\n",
    "scores = result.outputs['detection_scores'].float_val\n",
    "no_dets = result.outputs['num_detections'].float_val\n",
    "\n",
    "print(boxes)\n",
    "# output \n",
    "[0.05715984106063843, 0.4511566460132599, 0.9412486553192139, 0.9734638929367065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(\"/home/thuc/Downloads/mscoco_label_map.pbtxt\")\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=90, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "img_ = copy.deepcopy(img)\n",
    "image_vis = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "    img_,\n",
    "    np.reshape(boxes, [len(boxes) // 4,4]),\n",
    "    np.squeeze(classes).astype(np.int32),\n",
    "    np.squeeze(scores),\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    line_thickness=2,\n",
    "    max_boxes_to_draw=12,\n",
    "    min_score_thresh=0.9,\n",
    "    skip_scores=False,\n",
    "    skip_labels=False,\n",
    "    skip_track_ids=False\n",
    ")\n",
    "plt.imshow(image_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: \n",
    "![Result_1](https://i.imgur.com/p8FTdN5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving multiple model \n",
    "\n",
    "* Tensorflow Serving support serving multiple model and automatic reload the newest version of each model. \n",
    "* We need to create <code> serving.config </code> with the model_path are absolute path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_list { \n",
    "  config {\n",
    "    name: 'model-1'\n",
    "    base_path: 'path-to-model1'\n",
    "    model_platform: \"tensorflow\",\n",
    "    model_version_policy {\n",
    "      specific {\n",
    "         versions: 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  config {\n",
    "    name: 'model-2'\n",
    "    base_path: 'path-to-model2'\n",
    "    model_platform: \"tensorflow\",\n",
    "    model_version_policy {\n",
    "      specific {\n",
    "         versions: 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  config {\n",
    "    name: 'model-3'\n",
    "    base_path: 'path-to-model3'\n",
    "    model_platform: \"tensorflow\",\n",
    "    model_version_policy {\n",
    "      specific {\n",
    "         versions: 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start tensorflow_model_server\n",
    "!tensorflow_model_server --port=8500 --rest_api_port=8501 --model_config_file=./temp_models/serving.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Inference time with Batching request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Serving support inference batching \n",
    "* **Server side**\n",
    "\n",
    "    **Model without Server-side batching**\n",
    "![Batching inference](https://i.imgur.com/NuETVHI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model with server-side Batching**\n",
    "\n",
    "![server-side-batching](https://i.imgur.com/A0VMCUl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable server-side batching by <code>batching_parameter.txt</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size { value: 32 }\n",
    "batch_timeout_micros { value: 5000 }\n",
    "\n",
    "#with max_batch_size is number of batch-size, i.e batch-size=32\n",
    "#batch_timeout_micros. Maximum timeout to create the batch-size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add <code>batching_parameter.txt</code> to docker container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "    --model_name=mnist-serving \\\n",
    "    --model_base_path=/home/thuc/phh_workspace/temp_models/serving \\\n",
    "    --enable_batching=true \\\n",
    "    --batching_parameters_file=/home/thuc/phh_workspace/temp_models/batching_parameters.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Serving with Docker/Docker compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Pull docker image and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "# step 2\n",
    "docker run --rm -p 8500:8500 -p 8501:8501 --mount type=bind,source=/home/thuc/phh_workspace/temp_models/serving,target=/models/mnist-serving -e MODEL_NAME=mnist-serving -t tensorflow/serving\n",
    "\n",
    "# with config file\n",
    "docker run --rm -p 8500:8500 -p 8501:8501 --mount type=bind,source=/home/thuc/phh_workspace/temp_models/serving,target=/models/mnist-serving --mount type=bind,source=/home/thuc/phh_workspace/temp_models/serving.config,target=/models/serving.config -t tensorflow/serving --model_config_file=/models/serving.config\n",
    "\n",
    "# step 3 - testing with curl\n",
    "curl localhost:8501/v1/models/mnist-serving\n",
    "# output\n",
    "# return OK\n",
    "{\n",
    " \"model_version_status\": [\n",
    "  {\n",
    "   \"version\": \"1\",\n",
    "   \"state\": \"AVAILABLE\",\n",
    "   \"status\": {\n",
    "    \"error_code\": \"OK\",\n",
    "    \"error_message\": \"\"\n",
    "   }\n",
    "  }\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a web API with Python Flask, tensorflow-serving-api, docker/docker compose and preprocessing data using gRPC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Preprocessing, gRPC request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import grpc\n",
    "\n",
    "from protos.tensorflow_serving.apis import predict_pb2\n",
    "from protos.tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "from protos.tensorflow.core.framework import (\n",
    "    tensor_pb2,\n",
    "    tensor_shape_pb2,\n",
    "    types_pb2\n",
    ")\n",
    "\n",
    "\n",
    "def convert_image(encoded_img, to_rgb=False):\n",
    "\n",
    "    if isinstance(encoded_img, str):\n",
    "        b64_decoded_image = base64.b64decode(encoded_img)\n",
    "    else:\n",
    "        b64_decoded_image = encoded_img\n",
    "\n",
    "    img_arr = np.fromstring(b64_decoded_image, np.uint8)\n",
    "\n",
    "    img = cv2.imdecode(img_arr, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def grpc_infer(img):\n",
    "\n",
    "    channel = grpc.insecure_channel(\"10.5.0.5:8500\")\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = \"mnist-serving\"\n",
    "    request.model_spec.signature_name = \"serving_default\"\n",
    "\n",
    "    if img.ndim == 3:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    tensor_shape = img.shape\n",
    "    dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape]  \n",
    "    tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims)  \n",
    "    tensor = tensor_pb2.TensorProto(  \n",
    "                  dtype=types_pb2.DT_FLOAT,\n",
    "                  tensor_shape=tensor_shape,\n",
    "                  float_val=img.reshape(-1))\n",
    "    request.inputs['input_image'].CopyFrom(tensor)  \n",
    "\n",
    "    try:\n",
    "        result = stub.Predict(request, 10.0)\n",
    "        result = result.outputs[\"y_pred\"].float_val\n",
    "        result = np.array(result).reshape((-1, 10))\n",
    "        result = np.argmax(result, axis=-1)\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **API endpoint, handle request, parse result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "from utils import grpc_infer, convert_image\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/api/mnist', methods=['POST'])\n",
    "def hello():\n",
    "    encoded_img = request.values['encoded_image']\n",
    "    img = convert_image(encoded_img)\n",
    "\n",
    "    result = grpc_infer(img)\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"code\": 200,\n",
    "            \"result\": result.tolist()\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host=\"10.5.0.4\", port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dockerfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM ubuntu:16.04\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y python3-pip python3-dev libglib2.0-0 libsm6 libxrender1 libxext6 \\\n",
    "    && cd /usr/local/bin \\\n",
    "    && ln -s /usr/bin/python3 python \\\n",
    "    && pip3 install --upgrade pip\n",
    "\n",
    "RUN mkdir /code\n",
    "WORKDIR /code\n",
    "COPY requirements.txt /code/requirements.txt\n",
    "RUN pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Docker compose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version: '2'\n",
    "services:\n",
    "  web:\n",
    "    container_name: mnist_api\n",
    "    build: .\n",
    "    restart: always\n",
    "    volumes:\n",
    "      - .:/code\n",
    "    command: bash -c \"python3 serve.py\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    networks:\n",
    "      mynet:\n",
    "        ipv4_address: 10.5.0.4\n",
    "  tf-serving:\n",
    "    image: tensorflow/serving\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"8500:8500\"\n",
    "      - \"8501:8501\"\n",
    "    volumes:\n",
    "      - ./serving:/models\n",
    "      - ./serving_docker.config:/models/serving_docker.config\n",
    "    command: --model_config_file=/models/serving_docker.config\n",
    "    networks:\n",
    "      mynet:\n",
    "        ipv4_address: 10.5.0.5\n",
    "\n",
    "networks:\n",
    "  mynet:\n",
    "    driver: bridge\n",
    "    ipam:\n",
    "     config:\n",
    "       - subnet: 10.5.0.0/16\n",
    "         gateway: 10.5.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Build image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "!docker-compose build\n",
    "\n",
    "# step 2\n",
    "!docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test API with Postman, data input as base64**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![api](https://i.imgur.com/Y3wWLiQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
